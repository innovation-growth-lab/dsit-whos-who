model_training:
  # Data splitting
  test_size: 0.1
  random_seed: 42

  # Base model parameters grid for XGBoost
  base_params:
    n_estimators: [100, 300, 500, 1000]
    max_depth: [3, 5, 7, 9, 11]
    learning_rate: [0.001, 0.01, 0.1]
    min_child_weight: [1, 3, 5, 7]
    gamma: [0, 0.1, 0.2, 0.3]
    subsample: [0.6, 0.8, 1.0]
    colsample_bytree: [0.6, 0.8, 1.0]
    scale_pos_weight: [1, 2, 5]  # Additional weight balancing
    max_delta_step: [0, 1, 5]    # Can help with class imbalance
    reg_alpha: [0, 0.1, 1.0]     # L1 regularization
    reg_lambda: [0, 0.1, 1.0]    # L2 regularization

  # SMOTE specific parameters
  smote:
    enabled: true
    k_neighbors: [3, 5, 7, 9, 11]

  # Cross validation
  cv:
    n_splits: 5
    shuffle: true

  # Grid search
  grid_search:
    scoring: "f1"  # Using F1 since we care about both precision and recall
    n_jobs: 8
    verbose: 2

model_prediction:
  model_choice: "class_weights_model"
  threshold: 0.8
